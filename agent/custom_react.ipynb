{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phi.nguyen\\.conda\\envs\\neurond\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "from typing import List, Optional\n",
    "\n",
    "# Third-party imports\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "from pprint import pprint\n",
    "\n",
    "# llama-index imports\n",
    "from llama_index.agent.openai import OpenAIAssistantAgent, OpenAIAgent\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool, ToolMetadata\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Local imports\n",
    "from ai_config_schema import LLM\n",
    "from get_llm import get_llm, token_counter, get_llm_azure\n",
    "from web_search import (\n",
    "    get_page_content,\n",
    "    process_search_results,\n",
    "    generate_queries_search_engine,\n",
    ")\n",
    "from llama_index.core.agent.react import ReActChatFormatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.react.base import ReActAgent\n",
    "from llama_index.core.agent.react.output_parser import ReActOutputParser\n",
    "from llama_index.core.base.llms.types import ChatResponse\n",
    "from llama_index.core.agent.react.types import ResponseReasoningStep\n",
    "from llama_index.core.agent.react.formatter import ReActChatFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReActOutputParser(ReActOutputParser):\n",
    "    def parse(self, output: str, is_streaming: bool = False) -> ResponseReasoningStep:\n",
    "        if \"Action:\" in output:\n",
    "            return self.parse_action_reasoning_step(output)\n",
    "\n",
    "        if \"Answer:\" in output:\n",
    "            thought, answer = self.extract_final_response(output)\n",
    "            return ResponseReasoningStep(\n",
    "                thought=thought, response=answer, is_streaming=is_streaming\n",
    "            )\n",
    "\n",
    "        if \"Thought:\" not in output:\n",
    "            return ResponseReasoningStep(\n",
    "                thought=\"(Implicit) I can answer without any more tools!\",\n",
    "                response=output,\n",
    "                is_streaming=True,  # Allow streaming for direct responses\n",
    "            )\n",
    "\n",
    "        raise ValueError(f\"Could not parse output: {output}\")\n",
    "\n",
    "\n",
    "class CustomReActAgent(ReActAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.output_parser = CustomReActOutputParser()\n",
    "\n",
    "    def _infer_stream_chunk_is_final(\n",
    "        self, chunk: ChatResponse, missed_chunks_storage: list\n",
    "    ) -> bool:\n",
    "        latest_content = chunk.message.content\n",
    "        if latest_content:\n",
    "            if \"Action:\" in latest_content:\n",
    "                return False  # Do not finalize if an action is detected\n",
    "            if \"Answer:\" in latest_content:\n",
    "                missed_chunks_storage.clear()\n",
    "                return True\n",
    "            if \"Thought\" not in latest_content:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "# Usage\n",
    "# Initialize your custom agent with the necessary parameters\n",
    "# custom_agent = CustomReActAgent(tools=your_tools, llm=your_llm, memory=your_memory, ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent with web tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "\n",
    "def run_agent(\n",
    "    model_name: str,\n",
    "    system_prompt: str,\n",
    "    tools: Optional[List] = None,\n",
    "    history: Optional[List] = None,\n",
    "):\n",
    "    if model_name in [\"gpt-4o-mini\", \"gpt-4-turbo\", \"gpt-4\"]:\n",
    "        agent = CustomReActAgent.from_tools(\n",
    "            tools=tools,\n",
    "            system_prompt=system_prompt,\n",
    "            chat_history=history,\n",
    "            llm=get_llm_azure(),\n",
    "            verbose=False,\n",
    "            max_iterations=10,\n",
    "            react_chat_formatter=ReActChatFormatter(context=system_prompt),\n",
    "        )\n",
    "    else:\n",
    "        agent = CustomReActAgent.from_tools(\n",
    "            tools=tools,\n",
    "            system_prompt=system_prompt,\n",
    "            chat_history=history,\n",
    "            llm=get_llm(model_name),\n",
    "            verbose=False,\n",
    "            max_iterations=10,\n",
    "            react_chat_formatter=ReActChatFormatter(context=system_prompt),\n",
    "        )\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from system_prompt import DEFAULT_SYSTEM_PROMPT, DEFAULT_SYSTEM_PROMPT_OPTIMIZED\n",
    "import datetime\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT_WITH_TIME = DEFAULT_SYSTEM_PROMPT.format(\n",
    "    date=str(datetime.datetime.now().date())\n",
    ")\n",
    "DEFAULT_SYSTEM_PROMPT_OPTIMIZED_WITH_TIME = DEFAULT_SYSTEM_PROMPT_OPTIMIZED.format(\n",
    "    date=str(datetime.datetime.now().date())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure tools\n",
    "web_search_tool = FunctionTool.from_defaults(\n",
    "    fn=process_search_results,\n",
    "    tool_metadata=ToolMetadata(\n",
    "        name=\"web_search_tool\",\n",
    "        description=(\n",
    "            f\"Used to retrieve information about up-to-date information, website or the information out of LLM's knowledge.'\"\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "web_fetch_tool = FunctionTool.from_defaults(\n",
    "    fn=get_page_content,\n",
    "    tool_metadata=ToolMetadata(\n",
    "        name=\"web_fetch_tool\",\n",
    "        description=(f\"Used to fetch information of specific url\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "tools = [web_fetch_tool, web_search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub question: ['kết quả trận T1 vs BLG mới nhất']\n",
      "Kết quả trận đấu gần đây nhất giữa T1 và BLG là T1 đã giành chiến thắng với tỷ số 2-1 trong trận tứ kết của giải đấu Esports World Cup 2024. Đây là một trận đấu khó khăn và kịch tính cho cả hai đội. T1 đã xuất sắc đánh bại BLG và trở thành đội đầu tiên giành vé vào bán kết của giải đấu. Trận đấu diễn ra rất căng thẳng với T1 thắng ván 1, BLG thắng ván 2, và T1 cuối cùng đã giành chiến thắng ở ván quyết định. Đây được xem là màn phục thù thành công của T1 sau thất bại trước BLG tại MSI 2024 trước đó."
     ]
    }
   ],
   "source": [
    "DEFAULT_SYSTEM_PROMPT_WITH_TIME = DEFAULT_SYSTEM_PROMPT.format(\n",
    "    date=str(datetime.datetime.now().date())\n",
    ")\n",
    "model_name = \"claude-medium\"\n",
    "agent = run_agent(\n",
    "    model_name=model_name,\n",
    "    system_prompt=DEFAULT_SYSTEM_PROMPT_WITH_TIME,\n",
    "    tools=tools,\n",
    "    history=\"\",\n",
    ")\n",
    "\n",
    "input = \"kết quả trận T1 vs BLG mới nhất\"\n",
    "\n",
    "resp = agent.stream_chat(input)\n",
    "\n",
    "for token in resp.response_gen:\n",
    "    print(token, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bạn vừa hỏi về \"kết quả trận T1 vs BLG mới nhất\". Cụ thể, bạn muốn biết kết quả của trận đấu gần đây nhất giữa hai đội T1 và BLG trong một giải đấu Esports."
     ]
    }
   ],
   "source": [
    "input = \"tôi vừa hỏi gì?\"\n",
    "resp = agent.stream_chat(input)\n",
    "for token in resp.response_gen:\n",
    "    print(token, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub question: ['Tổng bí thư Việt Nam hiện nay là ai']\n",
      "Theo thông tin mới nhất, Tổng Bí thư Ban Chấp hành Trung ương Đảng Cộng sản Việt Nam hiện nay là đồng chí Tô Lâm. Ông được bầu làm Tổng Bí thư tại Hội nghị Trung ương khóa XIII bất thường vào ngày 3 tháng 8 năm 2024, thay thế đồng chí Nguyễn Phú Trọng đã mất khi đang tại nhiệm. Đồng chí Tô Lâm là Đại tướng Công an nhân dân Việt Nam và trước đó đã từng giữ chức vụ Chủ tịch nước từ tháng 5 đến tháng 10 năm 2024."
     ]
    }
   ],
   "source": [
    "input = \"Tổng bí thư là ai??\"\n",
    "resp = agent.stream_chat(input)\n",
    "for token in resp.response_gen:\n",
    "    print(token, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTôi xin lỗi, tôi đã cung cấp thông tin không chính xác trong câu trả lời trước. Tôi sẽ kiểm tra lại thông tin mới nhất về Chủ tịch nước Việt Nam.\n",
      "\n",
      "Thought: Tôi cần tìm kiếm thông tin mới nhất về Chủ tịch nước Việt Nam hiện tại. Tôi sẽ sử dụng công cụ tìm kiếm web để có thông tin chính xác và cập nhật.\n",
      "\n",
      "Action: web_search_tool\n",
      "Action Input: {\"input\": \"Chủ tịch nước Việt Nam hiện nay\"}"
     ]
    }
   ],
   "source": [
    "input = \"Thế còn chủ tịch nước?\"\n",
    "resp = agent.stream_chat(input)\n",
    "for token in resp.response_gen:\n",
    "    print(token, end=\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
